{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "from enum import Enum\n",
    "\n",
    "#methodology; TLDR: idempotent write op with job-id as primary key\n",
    "# 1. use cvs as database\n",
    "# 2. load csv in cache\n",
    "# 3. scrape & parse linkedin jobs\n",
    "# 4. look up jobid, omit if exists in cache, add to cache and jobs_to_add array\n",
    "# 5. write jobs_to_add back to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurations\n",
    "#base_url_prefix = \"https://www.linkedin.com/jobs/search?keywords=software%20engineer%20OR%20engineering%20manager&location=United%20States&pageNum=0&start=\"\n",
    "base_url_prefix = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=software%20engineer%20OR%20engineering%20manager&location=United%2BStates&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&start=\"\n",
    "row_increment_default = 10\n",
    "max_row_default = 2000\n",
    "csv_name=\"linkedin-job-scraper-database.csv\"\n",
    "csv_columns=['job_id','company','job_type','title','location','link','date']\n",
    "\n",
    "class DebugLevel(Enum):\n",
    "    WARN = 0\n",
    "    GENERAL = 1\n",
    "    GRANULAR = 2\n",
    "    \n",
    "debug_level = DebugLevel.WARN\n",
    "debug_company=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializations\n",
    "jobs={} # updated cache, won't be written back to csv\n",
    "jobs_to_add=[] # write-back as additions, strictly as a write buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_in_cache(reader):\n",
    "    dups_detection=[]\n",
    "    for row in reader: #reader knows first row is headers\n",
    "        key = row['job_id']\n",
    "        \n",
    "        if key in dups_detection:\n",
    "            print(f\"duplicate detected: job_id={key}\")\n",
    "        else:\n",
    "            dups_detection.append(key)\n",
    "            \n",
    "        value = {k: v for k, v in row.items() if k != 'job_id'}\n",
    "        jobs[key] = value\n",
    "\n",
    "    print(f\"{len(jobs)} rows read from csv\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555 rows read from csv\n"
     ]
    }
   ],
   "source": [
    "# load csv into cache\n",
    "if not os.path.exists(csv_name):\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile) #this will create new file if not exist\n",
    "        writer.writerow(csv_columns)\n",
    "else:\n",
    "    with open(csv_name, 'r+', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        load_csv_in_cache(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,max_row_default,row_increment_default):\n",
    "    base_url = f\"{base_url_prefix}{i}\"\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        job_listings = soup.find_all('div', {'class':'job-search-card'})\n",
    "        for job in job_listings:\n",
    "            try:\n",
    "                date = job.find('time', {'class':'job-search-card__listdate'}).attrs['datetime']\n",
    "            except AttributeError:\n",
    "                date = job.find('time', {'class':'job-search-card__listdate--new'}).attrs['datetime']\n",
    "            \n",
    "            title = job.find('h3', {'class': 'base-search-card__title'}).text.strip()\n",
    "            job_type = 'em' if 'manager' in title.lower() else 'eng'\n",
    "            company = job.find('a', {'class': 'hidden-nested-link'}).text.strip()\n",
    "            location = job.find('span', {'class': 'job-search-card__location'}).text.strip()\n",
    "            link = job.find('a', {'class': 'base-card__full-link'}).attrs['href']\n",
    "            pattern = r\"(.*-)(\\d+)\"\n",
    "            job_id = re.search(pattern,link).group(2)\n",
    "\n",
    "            #debug\n",
    "            if debug_level==DebugLevel.GRANULAR and debug_company != \"\" and company == debug_company:\n",
    "                print(f\"id:{job_id},title:{title},job_type:{job_type},link:{link}\")\n",
    "            elif debug_level==DebugLevel.GENERAL:\n",
    "                print(f\"id:{job_id},title:{title},job_type:{job_type},company:{company}\")\n",
    "            \n",
    "            #look up job_id, omit if cache hit, add to cache and jobs_to_add array if cache miss\n",
    "            if len(jobs_to_add) > 0 and job_id in jobs_to_add[0]: # exists in delta\n",
    "                if debug_level==DebugLevel.WARN: #if item offset is set efficiently this printline should never show\n",
    "                    print(f\"we've seen {company}-{job_id}\")\n",
    "                continue\n",
    "            elif jobs.get(job_id,\"\") != \"\": # cache hit: exists in database\n",
    "                continue\n",
    "            else:                    \n",
    "                jobs.setdefault(job_id,{\n",
    "                    csv_columns[1]: company,\n",
    "                    csv_columns[2]: job_type,\n",
    "                    csv_columns[3]: title,\n",
    "                    csv_columns[4]: location,\n",
    "                    csv_columns[5]: link,\n",
    "                    csv_columns[6]: date\n",
    "                })\n",
    "                jobs_to_add.append([job_id,company,job_type,title,location,link,date])\n",
    "                \n",
    "    else:\n",
    "        print(\"Failed to fetch job listings.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 jobs added\n"
     ]
    }
   ],
   "source": [
    "with open(csv_name, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Append all new rows\n",
    "    writer.writerows(jobs_to_add)\n",
    "    print(f\"{len(jobs_to_add)} jobs added\")\n",
    "jobs_to_add=[] #reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SynergisticIT</th>\n",
       "      <td>15</td>\n",
       "      <td>2024-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICONMA</th>\n",
       "      <td>11</td>\n",
       "      <td>2024-05-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jobs Malaysia - Two95 HR HUB</th>\n",
       "      <td>8</td>\n",
       "      <td>2024-04-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Varo Bank</th>\n",
       "      <td>8</td>\n",
       "      <td>2024-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lockheed Martin</th>\n",
       "      <td>8</td>\n",
       "      <td>2024-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Airbnb</th>\n",
       "      <td>7</td>\n",
       "      <td>2024-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Microsoft</th>\n",
       "      <td>7</td>\n",
       "      <td>2024-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Team Remotely Inc</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genesis10</th>\n",
       "      <td>6</td>\n",
       "      <td>2024-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marlee (Fingerprint For Success)</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nike</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intellectt Inc</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Get It Recruit - Information Technology</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interclypse</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-05-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Artera</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oracle</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tagup, Inc.</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Home Depot</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinkedIn</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experian</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-05-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         count         max\n",
       "company                                                   \n",
       "SynergisticIT                               15  2024-05-20\n",
       "ICONMA                                      11  2024-05-15\n",
       "Jobs Malaysia - Two95 HR HUB                 8  2024-04-26\n",
       "Varo Bank                                    8  2024-05-10\n",
       "Lockheed Martin                              8  2024-05-20\n",
       "Airbnb                                       7  2024-05-16\n",
       "Microsoft                                    7  2024-05-21\n",
       "Team Remotely Inc                            6  2024-05-21\n",
       "Genesis10                                    6  2024-05-21\n",
       "Marlee (Fingerprint For Success)             5  2024-05-17\n",
       "Nike                                         5  2024-05-21\n",
       "Intellectt Inc                               5  2024-05-17\n",
       "Get It Recruit - Information Technology      5  2024-05-20\n",
       "Interclypse                                  5  2024-05-21\n",
       "Artera                                       4  2024-04-25\n",
       "Oracle                                       4  2024-05-20\n",
       "Tagup, Inc.                                  4  2024-05-10\n",
       "The Home Depot                               4  2024-05-16\n",
       "LinkedIn                                     4  2024-05-14\n",
       "Experian                                     4  2024-05-20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(jobs).T\n",
    "df2=df.groupby(['company'])['date'].agg(['count','max'])\n",
    "df2.sort_values('count',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo connect to google drive via api"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
