{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "\n",
    "#methodology; TLDR: idempotent write op with job-id as primary key\n",
    "# 1. use cvs as database\n",
    "# 2. load csv in cache\n",
    "# 3. scrape & parse linkedin jobs\n",
    "# 4. look up jobid, omit if exists in cache, add to cache and jobs_to_add array\n",
    "# 5. write jobs_to_add back to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurations\n",
    "base_url_prefix = \"https://www.linkedin.com/jobs/search?keywords=software%20engineer%20OR%20engineering%20manager&location=United%20States&pageNum=0&start=\"\n",
    "row_increment_default = 25\n",
    "max_row_default = 501\n",
    "debug_mode = False\n",
    "debug_company=\"\"\n",
    "csv_name=\"linkedin-job-scraper-database.csv\"\n",
    "csv_columns=['job_id','company','job_type','title','location','link','date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializations\n",
    "jobs={} # updated cache, won't be written back to csv\n",
    "jobs_to_add=[] # write-back as additions, strictly as a write buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_in_cache(reader):\n",
    "    for row in reader: #reader knows first row is headers\n",
    "        key = row['job_id']\n",
    "        value = {k: v for k, v in row.items() if k != 'job_id'}\n",
    "        jobs[key] = value\n",
    "\n",
    "    print(f\"{len(jobs)} rows read from csv\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 rows read from csv\n"
     ]
    }
   ],
   "source": [
    "# load csv into cache\n",
    "if not os.path.exists(csv_name):\n",
    "    with open(csv_name, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile) #this will create new file if not exist\n",
    "        writer.writerow(csv_columns)\n",
    "else:\n",
    "    with open(csv_name, 'r+', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        load_csv_in_cache(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,max_row_default,row_increment_default):\n",
    "    base_url = f\"{base_url_prefix}{i}\"\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        job_listings = soup.find_all('div', {'class':'job-search-card'})\n",
    "        for job in job_listings:\n",
    "            try:\n",
    "                date = job.find('time', {'class':'job-search-card__listdate'}).attrs['datetime']\n",
    "            except AttributeError:\n",
    "                date = job.find('time', {'class':'job-search-card__listdate--new'}).attrs['datetime']\n",
    "            \n",
    "            title = job.find('h3', {'class': 'base-search-card__title'}).text.strip()\n",
    "            job_type = 'em' if 'manager' in title.lower() else 'eng'\n",
    "            company = job.find('a', {'class': 'hidden-nested-link'}).text.strip()\n",
    "            location = job.find('span', {'class': 'job-search-card__location'}).text.strip()\n",
    "            link = job.find('a', {'class': 'base-card__full-link'}).attrs['href']\n",
    "            pattern = r\"(.*-)(\\d+)\"\n",
    "            job_id = re.search(pattern,link).group(2)\n",
    "\n",
    "            #debug\n",
    "            if debug_mode and debug_company != \"\" and company == debug_company:\n",
    "                print(f\"id:{job_id},title:{title},job_type:{job_type},link:{link}\")\n",
    "            \n",
    "            #look up job_id, omit if cache hit, add to cache and jobs_to_add array if cache miss\n",
    "            if jobs.get(job_id,'') != '': #cache hit\n",
    "                if debug_mode:\n",
    "                    print(f\"we've seen {company}-{job_id}\")\n",
    "                continue\n",
    "            else:                    \n",
    "                jobs.setdefault(job_id,{\n",
    "                    csv_columns[1]: company,\n",
    "                    csv_columns[2]: job_type,\n",
    "                    csv_columns[3]: title,\n",
    "                    csv_columns[4]: location,\n",
    "                    csv_columns[5]: link,\n",
    "                    csv_columns[6]: date\n",
    "                })\n",
    "                jobs_to_add.append([job_id,company,job_type,title,location,link,date])\n",
    "                \n",
    "    else:\n",
    "        print(\"Failed to fetch job listings.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 jobs added\n"
     ]
    }
   ],
   "source": [
    "with open(csv_name, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Append all new rows\n",
    "    writer.writerows(jobs_to_add)\n",
    "    print(f\"{len(jobs_to_add)} jobs added\")\n",
    "jobs_to_add=[] #reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincentchen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SynergisticIT</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nike</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ashby</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genesis Therapeutics</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-04-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ever</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Walt Disney Company</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netflix</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open Systems Inc.</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lockheed Martin</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinkedIn</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Saragossa</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tagup, Inc.</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Midpoint Markets</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Airbnb</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count         max\n",
       "company                                   \n",
       "SynergisticIT                4  2024-05-20\n",
       "Nike                         3  2024-05-17\n",
       "Ashby                        3  2024-05-01\n",
       "Genesis Therapeutics         3  2024-04-22\n",
       "Ever                         2  2024-05-16\n",
       "The Walt Disney Company      2  2024-05-18\n",
       "Netflix                      2  2024-05-20\n",
       "Open Systems Inc.            2  2024-05-15\n",
       "Lockheed Martin              2  2024-05-20\n",
       "LinkedIn                     2  2024-05-14\n",
       "Saragossa                    2  2024-05-13\n",
       "Tagup, Inc.                  2  2024-05-10\n",
       "Midpoint Markets             2  2024-05-11\n",
       "Airbnb                       2  2024-05-16"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(jobs).T\n",
    "df2=df.groupby(['company'])['date'].agg(['count','max'])\n",
    "df2.sort_values('count',ascending=False)[df2['count']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo connect to google drive via api"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
